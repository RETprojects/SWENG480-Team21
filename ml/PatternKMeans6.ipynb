{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This notebook analyzes the efficacy of different clustering algorithms for matching design patterns and design problems.\n",
        "# Note: Table C2 in Hussain et al 2017 seems to indicate that fuzzy c-means clustering with binary weighting is the most effective combination for GoF patterns,\n",
        "# but TF-IDF yields the highest f-value for fuzzy c-means (0.73).\n",
        "# We are aiming for an f-value of 0.7 or above.\n",
        "\n",
        "!pip install unidecode\n",
        "!pip install fuzzy-c-means\n",
        "!pip install --upgrade scikit-learn\n",
        "!pip install scikit-learn-extra\n",
        "\n",
        "from fcmeans                          import FCM\n",
        "\n",
        "# Data Structures\n",
        "import numpy  as np\n",
        "import pandas as pd\n",
        "import json\n",
        "# Corpus Processing\n",
        "import re\n",
        "import nltk\n",
        "import nltk.corpus\n",
        "from nltk.tokenize                    import word_tokenize\n",
        "from nltk.stem                        import WordNetLemmatizer\n",
        "from nltk                             import SnowballStemmer, PorterStemmer\n",
        "nltk.download('punkt')\n",
        "\n",
        "from sklearn.feature_extraction.text  import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.preprocessing            import normalize, Normalizer\n",
        "from sklearn.decomposition            import PCA, TruncatedSVD\n",
        "from sklearn.cluster                  import KMeans, BisectingKMeans, AgglomerativeClustering\n",
        "from sklearn_extra.cluster            import KMedoids\n",
        "from sklearn.pipeline                 import make_pipeline\n",
        "\n",
        "from unidecode                        import unidecode\n",
        "\n",
        "# K-Means\n",
        "from sklearn                          import cluster\n",
        "\n",
        "# Visualization and Analysis\n",
        "import matplotlib.pyplot  as plt\n",
        "import matplotlib.cm      as cm\n",
        "import seaborn            as sns\n",
        "from sklearn.metrics                  import silhouette_samples, silhouette_score, confusion_matrix, ConfusionMatrixDisplay, f1_score\n",
        "from wordcloud                        import WordCloud\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "np.random.seed(9)\n"
      ],
      "metadata": {
        "id": "lz6NJ12FIiXJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90b93a89-2550-43ca-e46b-44035a28dd8f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 KB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fuzzy-c-means\n",
            "  Downloading fuzzy_c_means-1.6.3-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: tabulate<0.9.0,>=0.8.9 in /usr/local/lib/python3.8/dist-packages (from fuzzy-c-means) (0.8.10)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from fuzzy-c-means) (1.22.4)\n",
            "Collecting typer<0.4.0,>=0.3.2\n",
            "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: pydantic<2.0.0,>=1.8.2 in /usr/local/lib/python3.8/dist-packages (from fuzzy-c-means) (1.10.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic<2.0.0,>=1.8.2->fuzzy-c-means) (4.5.0)\n",
            "Collecting click<7.2.0,>=7.1.1\n",
            "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: click, typer, fuzzy-c-means\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.3\n",
            "    Uninstalling click-8.1.3:\n",
            "      Successfully uninstalled click-8.1.3\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.7.0\n",
            "    Uninstalling typer-0.7.0:\n",
            "      Successfully uninstalled typer-0.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flask 2.2.3 requires click>=8.0, but you have click 7.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed click-7.1.2 fuzzy-c-means-1.6.3 typer-0.3.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.2.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "Successfully installed scikit-learn-1.2.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikit-learn-extra\n",
            "  Downloading scikit_learn_extra-0.2.0-cp38-cp38-manylinux2010_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn-extra) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn-extra) (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn-extra) (1.22.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (3.1.0)\n",
            "Installing collected packages: scikit-learn-extra\n",
            "Successfully installed scikit-learn-extra-0.2.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getTrainData(fileName):\n",
        "  df = pd.read_csv(fileName)\n",
        "  df = df.drop_duplicates(subset=['name'])\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "ijpPxbCoPFKa"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the design problem to the dataset.\n",
        "#problemRow = {'name':\"bridge design problem 1\", 'correct_category':1, 'overview':\"Design a system enabling to display on a screen some empty windows (no button, no menu…). A window can have several different styles depending on the platform used. We consider two platforms, XWindow and Presentation Manager. The client code must be written independently and without knowledge of the future execution platform. It is probable that the system evolves in order to display specialized windows by ‘application windows’ (able to manage applications) and ‘iconised windows’ (with an icon)\"}\n",
        "#problemRow = {'name':\"state design problem 2\", 'correct_category':0,  'overview':\"Design a DVD market place work. The DVD marketplace provides DVD to its clients with three categories: children, normal and new. A DVD is new during some weeks, and after change category. The DVD price depends on the category. It is probable that the system evolves in order to take into account the horror category\"}\n",
        "#problemRow = {'name':\"mediator design problem 3\", 'correct_category':0,  'overview':\"Design the communications of a plane approaching an airport. When a plane approaches an airport, it must announce to all the other planes which are around that it intends to land, and await their confirmation before carrying out the operation. It is the control tower of the airport which guarantees the regulation of the air traffic, by making sure that there is no trajectory or destination conflict between several planes. In addition to the class diagram, you must also submit a collaboration (in the form of a diagram of collaboration or a diagram of objects and sequence) that describes the landing of a plane amidst in a context of two demands to land and one wanting to take off\"}\n",
        "#problemRow = {'name':\"composite design problem 4\", 'correct_category':1, 'overview':\"Design a system enabling you to draw a graphic image. A graphic image is composed of lines, rectangles, texts, and images. An image may be composed of other images, lines, rectangles, and texts\"}\n",
        "#problemRow = {'name':\"decorator design problem 5\", 'correct_category':1, 'overview':\"Design a system enabling you to display visual objects on a screen. A visual object can be composed of one or more texts or images. If needed, the system must allow the addition of a vertical scroll bar, a horizontal scroll bar, an edge and a menu to this object. These additions may be accumulated.\"}\n",
        "#problemRow = {'name':\"chain of responsbility design problem 6\", 'correct_category':0, 'overview':\"Design a help manager for a Java application. A help manager allows the display of a help message depending on the objects on which a client has clicked. For example, the “?”, sometimes located near the contextual menu of a Windows dialog box, allows the display of the help related to the button or the area where to click. If the button on which one clicks does not contain help, it is the area container which displays its help, and so on. If any object contains help, the help manager displays /“No help available for this area/”. Instantiate your class diagram in a sequence diagram of the example of a printing window. This window (JDialog) consists in an explanatory text (JLabel) and in a container (JPanel). This last contains a /“Print button/” (JButton) and a /“Cancel button/” (JButton). The /“Print button/” contains help /“Launches the impression of the document/”. The /“Cancel button/” the text as well as the window do not contain help. Lastly, the container contains help /“Click on one of the buttons/”. In the sequence diagram, reveal the scenarios: /“The user asks for the help of the Print button/”, /“the user asks for the help of the Cancel button/”, and /“the user asks for the help of the text/”\"}\n",
        "#problemRow = {'name':\"command design problem 7\", 'correct_category':0, 'overview':\"Design a tutorial to learn how to program a calculator. This calculator executes the four basic arithmetic operations. The goal of this tutorial is to make it possible to take a set of operations to be executed sequentially. The tutorial presents a button for each arithmetic operation, and two input fields for the operands. After each click on a button of an operation, the user has then the choice to start again or execute the sequence of operations to obtain the result. It is probable that this tutorial evolves in order to make it possible for the user to remove the last operation of the list and to take into account the operation of modulo\"}\n",
        "#problemRow = {'name':\"visitor design problem 8\", 'correct_category':0, 'overview':\"Many distinct and unrelated operations need to be performed on node objects in a heterogeneous aggregate structure. You want to avoid ‘polluting’ the node classes with these operations. And, you don't want to have to query the type of each node and cast the pointer to the correct type before performing the desired operation.\"}\n",
        "#problemRow = {'name':\"adapter design problem 9\", 'correct_category':1, 'overview':\"Design a drawing editor. A design is composed of te graphics (lines, rectangles and roses), positioned at precise positions. Each graphic form must be modeled by a class that provides a method draw(): void. A rose is a complex graphic designed by a black-box class component. This component performs this drawing in memory, and provides access through a method getRose(): int that returns the address of the drawing. It is probable that the system evolves in order to draw circles\"}\n",
        "# TODO: we need some design problems related to creational patterns\n",
        "#df = df.append(problemRow, ignore_index=True)\n"
      ],
      "metadata": {
        "id": "YmdAtfnSxdoX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removes a list of words (ie. stopwords) from a tokenized list.\n",
        "def removeWords(listOfTokens, listOfWords):\n",
        "    return [token for token in listOfTokens if token not in listOfWords]\n",
        "\n",
        "# applies stemming to a list of tokenized words\n",
        "def applyStemming(listOfTokens, stemmer):\n",
        "    return [stemmer.stem(token) for token in listOfTokens]\n",
        "\n",
        "# removes any words composed of less than 2 or more than 21 letters\n",
        "def twoLetters(listOfTokens):\n",
        "    twoLetterWord = []\n",
        "    for token in listOfTokens:\n",
        "        if len(token) <= 2 or len(token) >= 21:\n",
        "            twoLetterWord.append(token)\n",
        "    return twoLetterWord"
      ],
      "metadata": {
        "id": "WCyzW6A8Ic3D"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "def processCorpus(corpus, language, stemmer):   \n",
        "    stopwords = nltk.corpus.stopwords.words(language)\n",
        "    param_stemmer = stemmer\n",
        "    \n",
        "    for document in corpus:\n",
        "        index = corpus.index(document)\n",
        "        corpus[index] = str(corpus[index]).replace(u'\\ufffd', '8')   # Replaces the ASCII '�' symbol with '8'\n",
        "        corpus[index] = corpus[index].replace(',', '')          # Removes commas\n",
        "        corpus[index] = corpus[index].rstrip('\\n')              # Removes line breaks\n",
        "        corpus[index] = corpus[index].casefold()                # Makes all letters lowercase\n",
        "        \n",
        "        corpus[index] = re.sub('\\W_',' ', corpus[index])        # removes specials characters and leaves only words\n",
        "        corpus[index] = re.sub(\"\\S*\\d\\S*\",\" \", corpus[index])   # removes numbers and words concatenated with numbers IE h4ck3r. Removes road names such as BR-381.\n",
        "        corpus[index] = re.sub(\"\\S*@\\S*\\s?\",\" \", corpus[index]) # removes emails and mentions (words with @)\n",
        "        corpus[index] = re.sub(r'http\\S+', '', corpus[index])   # removes URLs with http\n",
        "        corpus[index] = re.sub(r'www\\S+', '', corpus[index])    # removes URLs with www\n",
        "\n",
        "        listOfTokens = word_tokenize(corpus[index])\n",
        "        twoLetterWord = twoLetters(listOfTokens)\n",
        "\n",
        "        listOfTokens = removeWords(listOfTokens, stopwords)\n",
        "        listOfTokens = removeWords(listOfTokens, twoLetterWord)\n",
        "        \n",
        "        listOfTokens = applyStemming(listOfTokens, param_stemmer)\n",
        "\n",
        "        corpus[index]   = \" \".join(listOfTokens)\n",
        "        corpus[index] = unidecode(corpus[index])\n",
        "\n",
        "    return corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNyvdEQ-IsoQ",
        "outputId": "d985d972-63d4-4151-8a49-0f280b644793"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: use k-means before this chunk of code to classify the problem with a pattern class, then perform cosine similarity with the problem and the list of candidate patterns from that class.\n",
        "# Source: https://danielcaraway.github.io/html/sklearn_cosine_similarity.html\n",
        "\n",
        "def cosine_sim(df, df_col, class_no, pos_to_last):\n",
        "  unigram_count = CountVectorizer(encoding='latin-1', binary=False)\n",
        "  unigram_count_stop_remove = CountVectorizer(encoding='latin-1', binary=False, stop_words='english')\n",
        "\n",
        "  # get the list of candidate patterns\n",
        "  txts = df_col.loc[df['Kmeans'] == class_no] # where label == class_no\n",
        "  vecs = unigram_count.fit_transform(txts)\n",
        "  \n",
        "  cos_sim = cosine_similarity(vecs[-pos_to_last], vecs)\n",
        "  #sim_sorted_doc_idx = cos_sim.argsort()\n",
        "  # print the most similar pattern to the problem; it's actually the problem itself\n",
        "  #print(\"Design Problem: \\n\" + txts.iloc[sim_sorted_doc_idx[-1][len(txts)-1]] + \"\\n\")\n",
        "\n",
        "  #bestFittingPatternDesc = txts.iloc[sim_sorted_doc_idx[-1][len(txts)-2]]\n",
        "\n",
        "  # print the second most similar pattern; it's likely the best-fitting design pattern for the design problem\n",
        "  #print(txts[sim_sorted_doc_idx[-1][len(txts)-2]])\n",
        "  #print(\"\\nCorrect Pattern: \" + (df['name'][(df['overview'] == bestFittingPatternDesc)]).to_string(index=False) + \"\\n\")\n",
        "\n",
        "  return cos_sim, txts\n"
      ],
      "metadata": {
        "id": "98Wc23fugsBk"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def displayPredictions(cos_sim, txts):\n",
        "  sim_sorted_doc_idx = cos_sim.argsort()\n",
        "  for i in range(len(txts) - 1):\n",
        "    patternDesc = txts.iloc[sim_sorted_doc_idx[-1][len(txts)-(i + 2)]]\n",
        "    patternName = (df['name'][(df['overview'] == patternDesc)]).to_string(index=False)\n",
        "    percentMatch = int((cos_sim[0][sim_sorted_doc_idx[-1][len(txts)-(i + 2)]]) * 100)\n",
        "    print(\"{}th pattern:  {:<20}{}%  match\".format(i+1, patternName, percentMatch))\n"
      ],
      "metadata": {
        "id": "M-VWv1XhUvQm"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def runAlgorithms(final_df, df):\n",
        "  # bisecting_strategy{“biggest_inertia”, “largest_cluster”}, default=”biggest_inertia”\n",
        "  final_df_array = final_df.to_numpy()\n",
        "\n",
        "  Bi_Bisect = BisectingKMeans(n_clusters=3, bisecting_strategy=\"biggest_inertia\")\n",
        "  Lc_Bisect = BisectingKMeans(n_clusters=3, bisecting_strategy=\"largest_cluster\")\n",
        "  Hierarchy = AgglomerativeClustering(n_clusters=3)\n",
        "  Fuzzy_Means = FCM(n_clusters=3)\n",
        "  Fuzzy_Means.fit(final_df_array)\n",
        "  kmed = KMedoids(n_clusters=3)\n",
        "  kmed_manhattan = KMedoids(n_clusters=3,metric='manhattan')\n",
        "  Kmeans = cluster.KMeans(n_clusters = 3)\n",
        "\n",
        "  Kmeans_labels = Kmeans.fit_predict(final_df)\n",
        "  fuzzy_labels = Fuzzy_Means.predict(final_df_array)\n",
        "  bi_bisect_labels = Bi_Bisect.fit_predict(final_df)\n",
        "  lc_bisect_labels = Lc_Bisect.fit_predict(final_df)  \n",
        "  hierarchy_labels = Hierarchy.fit_predict(final_df)\n",
        "  kmed_labels = kmed.fit_predict(final_df)\n",
        "  kmed_man_labels = kmed_manhattan.fit_predict(final_df)\n",
        "\n",
        "  df['Kmeans'] = Kmeans_labels\n",
        "  df['fuzzy'] = fuzzy_labels\n",
        "  df['hierarchy'] = hierarchy_labels\n",
        "  df['Bi_Bisect'] = bi_bisect_labels  \n",
        "  df['Lc_Bisect'] = lc_bisect_labels\n",
        "  df['PAM-EUCLIDEAN'] = kmed_labels\n",
        "  df['PAM-MANHATTAN'] = kmed_man_labels\n"
      ],
      "metadata": {
        "id": "zhxsGG5oQ-M0"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validateInput(designProblem):\n",
        "  numOfWords = len(designProblem.split())\n",
        "  if (numOfWords < 30 or numOfWords > 120):\n",
        "    return False\n",
        "  return True\n",
        "\n",
        "def main():\n",
        "  language = 'english'\n",
        "  stemmer = PorterStemmer()\n",
        "  vectorizer = TfidfVectorizer(sublinear_tf=True)\n",
        "  df = getTrainData(\"GOF Patterns (2.0).csv\")\n",
        "\n",
        "  while(True):\n",
        "    designProblem = input(\"Enter your design problem: \\n\")\n",
        "\n",
        "    if(not validateInput(designProblem)):\n",
        "      print(\"Invalid input size! please try again. \\n\")\n",
        "      continue\n",
        "\n",
        "    problemRow = {'name':\"design problem\", 'correct_category':4, 'overview': designProblem}\n",
        "    df = df.append(problemRow, ignore_index=True)\n",
        "\n",
        "    corpus = df['overview'].tolist()\n",
        "    corpus = processCorpus(corpus, language, stemmer)\n",
        "\n",
        "    X = vectorizer.fit_transform(corpus)\n",
        "    tf_idf = pd.DataFrame(data = X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "    runAlgorithms(tf_idf, df)\n",
        "\n",
        "    cos_sim, txts = cosine_sim(df, df['overview'], df['Kmeans'].iloc[df.index[-1]], 1)\n",
        "    displayPredictions(cos_sim, txts)\n",
        "    break\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkshK0w6W7vm",
        "outputId": "86cd44b8-6b4d-458b-eaf5-7e457643e829"
      },
      "execution_count": 32,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your design problem: \n",
            "Design a system enabling to display on a screen some empty windows (no button, no menu…). A window can have several different styles depending on the platform used. We consider two platforms, XWindow and Presentation Manager. The client code must be written independently and without knowledge of the future execution platform. It is probable that the system evolves in order to display specialized windows by ‘application windows’ (able to manage applications) and ‘iconised windows’ (with an icon)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1th pattern:  bridge              54%  match\n",
            "2th pattern:  abstract factory    43%  match\n",
            "3th pattern:  facade              41%  match\n",
            "4th pattern:  adapter             29%  match\n"
          ]
        }
      ]
    }
  ]
}